---
---

@inproceedings{yang2025transformer,
  title={Transformer-based Speech Model Learns Well as Infants and Encodes Abstractions through Exemplars in the Poverty of the Stimulus Environment},
  author={Yang*, Yi and Wang*, Yiming and Yuan, Jiahong},
  booktitle={Proceedings of the 31st International Conference on Computational Linguistics},
  pdf={https://aclanthology.org/2025.coling-main.528.pdf},
  code={https://github.com/YiYang-github/ToneLab},
  abbr={COLING},
  award={Oral},
  honor={Oral Presentation},
  year={2025},
  selected={true}
}


@inproceedings{yi2024automatic,
  title={Automated Tone Transcription and Clustering with Tone2Vec},
  author={Yang, Yi and Wang, Yiming and Yuan, Jiahong},
  abbr={EMNLP},
  award={Findings},
  year={2024},  
  booktitle={Findings of the Association for Computational Linguistics: EMNLP, 2024.},
  pdf={https://arxiv.org/abs/2410.02324},
  code={https://github.com/YiYang-github/ToneLab},
  selected={true}
}

@inproceedings{jiancan2023gif,
  title={GIF: A General Graph Unlearning Strategy via Influence Function},
  author={Wu*, Jiancan and Yang*, Yi and Qian, Yuchun and Sui, Yongduo and Wang, Xiang and He, Xiangnan},
  abbr={TheWebConf},
  pdf={https://arxiv.org/pdf/2304.02835},
  year={2023},
  abstract={With the greater emphasis on privacy and security in our society, the problem of graph unlearning ‚Äî revoking the influence of specific data on the trained GNN model, is drawing increasing attention. However, ranging from machine unlearning to recently emerged graph unlearning methods, existing efforts either resort to retraining paradigm, or perform approximate erasure that fails to consider the inter-dependency between connected neighbors or imposes constraints on GNN structure, therefore hard to achieve satisfying performance-complexity trade-offs. In this work, we explore the influence function tailored for graph unlearning, so as to improve the unlearning efficacy and efficiency for graph unlearning. We first present a unified problem formulation of diverse graph unlearning tasks w.r.t. node, edge, and feature. Then, we recognize the crux to the inability of traditional influence function for graph unlearning, and devise Graph Influence Function (GIF), a model-agnostic unlearning method that can efficiently and accurately estimate parameter changes in response to a ùúñ-mass perturbation in deleted data. The idea is to supplement the objective of the traditional influence function with an additional loss term of the influenced neighbors due to the structural dependency. Further deductions on the closed-form solution of parameter changes provide a better understanding of the unlearning mechanism. We conduct extensive experiments on four representative GNN models and three benchmark datasets to justify the superiority of GIF for diverse graph unlearning tasks in terms of unlearning efficacy, model utility, and unlearning efficiency.},
  booktitle={the International World Wide Web Conference, 2023.},
  code={https://github.com/wujcan/GIF-torch/},
  selected={true}
}



@inproceedings{yi2023how,
  title={DImBench: Missing Data Imputation Benchmark for Relational Databases},
  author={Zhang*, Zizhao and Yang*, Yi and Zou*, Lutong and Wen*, He and You, Jiaxuan },
  year={2023},  
  booktitle={Arxiv, 2023.},
  abbr={Manuscript},
}








