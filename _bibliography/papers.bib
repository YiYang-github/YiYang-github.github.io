---
---

@inproceedings{yi2024automatic,
  title={Automatic Transcription and Representations for Lexical Tones in Sino-Tibetan Languages},
  author={Yang, Yi and Wang, Yiming and Yuan, Jiahong},
  abbr={IC<sup>2</sup>S<sup>2</sup>},
  year={2024},  
  booktitle={the 10th International Conference on Computational Social Science, 2024.},
  selected={true}
}

@inproceedings{yi2024how,
  title={Saving Voices: How AI Can Rescue Endangered Languages in the Digital World},
  author={Yang, Yi and Wang, Yiming and Yuan, Jiahong},
  abbr={ICA, Beijing},
  honor={15-min Presentation},
  year={2024},  
  booktitle={the 74th Annual ICA Conference Beijing Regional Hub, 2024.<br> In the 4th Computational Social Science Research Methods Forum.},
  selected={true}
}

@inproceedings{yi2024how,
  title={Quantifying Language Evolution with Transcriptions Only},
  author={Yang, Yi and Wang, Yiming and Yang, Jiawei and Zhang, Mingjie and Yuan, Jiahong},
  abbr={CIEL},
  year={2024},  
  booktitle={the 15th International Conference in Evolutionary Linguistics, 2024.}
}

@inproceedings{yi2023how,
  title={DImBench: Missing Data Imputation Benchmark for Relational Databases},
  author={Zhang*, Zizhao and Yang*, Yi and Zou*, Lutong and Wen*, He and You, Jiaxuan },
  year={2023},  
  booktitle={Arxiv, 2023.},
  abbr={Manuscript},
}


@inproceedings{jiancan2023gif,
  title={GIF: A General Graph Unlearning Strategy via Influence Function},
  author={Wu*, Jiancan and Yang*, Yi and Qian, Yuchun and Sui, Yongduo and Wang, Xiang and He, Xiangnan},
  abbr={WWW},
  pdf={https://arxiv.org/pdf/2304.02835},
  year={2023},
  abstract={With the greater emphasis on privacy and security in our society, the problem of graph unlearning ‚Äî revoking the influence of specific data on the trained GNN model, is drawing increasing attention. However, ranging from machine unlearning to recently emerged graph unlearning methods, existing efforts either resort to retraining paradigm, or perform approximate erasure that fails to consider the inter-dependency between connected neighbors or imposes constraints on GNN structure, therefore hard to achieve satisfying performance-complexity trade-offs. In this work, we explore the influence function tailored for graph unlearning, so as to improve the unlearning efficacy and efficiency for graph unlearning. We first present a unified problem formulation of diverse graph unlearning tasks w.r.t. node, edge, and feature. Then, we recognize the crux to the inability of traditional influence function for graph unlearning, and devise Graph Influence Function (GIF), a model-agnostic unlearning method that can efficiently and accurately estimate parameter changes in response to a ùúñ-mass perturbation in deleted data. The idea is to supplement the objective of the traditional influence function with an additional loss term of the influenced neighbors due to the structural dependency. Further deductions on the closed-form solution of parameter changes provide a better understanding of the unlearning mechanism. We conduct extensive experiments on four representative GNN models and three benchmark datasets to justify the superiority of GIF for diverse graph unlearning tasks in terms of unlearning efficacy, model utility, and unlearning efficiency.},
  booktitle={the International World Wide Web Conference, 2023.},
  code={https://github.com/wujcan/GIF-torch/},
  selected={true}
}





